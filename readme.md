***Почему k8s - это круто?***  
  
Потому что он предоставляет перечень полезных функций для работы кластера какой-либо системы, а именно:  
1. **Service discovery and load balancing** - позволяет распределять сетевой трафик между различными подами (объединение контейнеров в рамках одной ноды). Т.е. если у нас, например, есть две worker-ноды с СУБД для базы данных "mars", на каждой из этих нод есть поды. На ноде №1 есть 2 пода, а на ноде №2 всего 1 под. К нам в систему приходит 300 однотипных запросов на запись в определенную базу данных в минуту, значит k8s под капотом распределит нагрузку в равном размере, т.е. по 100 запросов на каждый под. Данное преимущество k8s похоже на преимущество представленное в пункте 4, но немного отличается по своей сути от него. По итогу использования данного функционала мы можем получить просто DNS-имя или определенный порт в кластере и обращаться только к нему, а k8s под капотом самостоятельно распределит трафик запросов между нодами и подами.
  
2. **Storage orchestration** - это возможность k8s автоматически управлять хранилищем данных для приложений. Т.е. функционал, позволяющий, в случае, если под, работающий с базой данных, по какой-то причине упал, подключить новую ноду к дисковому массиву на котором была развернута необходимая нам база данных, при восстановлении пода на этой ноде (Self-healing). Т.е. это функция Kubernetes, которая автоматически управляет хранилищем данных, обеспечивая доступ к данным для приложений, даже если поды перезапускаются или перемещаются между нодами.
  
3. **Automated rollouts and rollbacks** - это возможность k8s автоматически управлять развертыванием новых версий приложений. Данный функционал позволяет "мягко" развертывать новые версии (контейнеры с ними). Происходит это примерно так: есть контейнер со старой версией, k8s поднимает контейнер с новой версией и если все работает, то он гасит контейнер со старой. Тем самым достигается бесшовный переход между двумя версиями приложения нашей системы. Также если новая версия вызывает сбои и ошибки, то k8s может автоматически откатить приложение к предыдущей версии для сохранения работоспособности кластера (rollouts - развертывание и rollbacks - откат).
  
4. **Automatic bin packing** - позволяет распределить контейнеры между ресурсами нашего кластера. Т.е. тут речь идет не о сетевой трафике как в пункте №1, а о эффективности использования ресурсов системы (железа). У нас есть две worker-ноды (на первой 2GB оперативной памяти, а на второй 6GB), на которых запущены по одному поду и в каждом поде по одному контейнеру для работы с СУБД. Если нам понадобилось запустить еще один такой же контейнер, требующий 1,8GB оперативной памяти, то k8s, под капотом, самостоятельно решит, что данный контейнер нужно разворачивать именно на второй ноде, исходя из ресурсов системы.
  
5. **Self-healing** - с помощью определенный процессов, k8s кластер мониторит состояние нод и лежащих на них подов и контейнеров. Следовательно, если одно из звеньев нашей системы по каким-либо причинам падает, то k8s запустит данный экземпляр повторно, тем самым сохранив логику и объем работы системы.
  
6. **Secret and configuration management** - функционал обеспечивающий надежность хранения чувствительных к утечкам данных (например, паролей, API-ключей, токенов и др.). Для целей хранения таких данных на master-ноде реализован специальный объект Secret, хранящий данные в зашифрованном виде. Поды кластера могут достучаться до данных, хранящихся в зашифрованном виде. При этом нет необходимости содержать такие данные в самих контейнерах, что неблагоприятно влияет на их безопасность. Также для настройки конфигурирования приложения k8s, хранит конфигурационные параметры на master-ноде в специльаном файле ConfigMaps. Поды в данном случае также могут достучаться до этих параметров. Следовательно, данный функционал позволяет обновить поведение приложения в некоторых случаях не переписывая его код.  
  
*Secret и ConfigMap хранятся в etcd — распределённой базе данных, которая является частью Kubernetes control plane. Это хранилище может быть расположено на мастер-нодах, но сами данные (Secrets и ConfigMaps) не хранятся прямо на нодах, они храняться в базе данных etcd, которая доступна всему кластеру.*